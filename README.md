# Papers to read

This is a list of all the papers that I like to read. These are more of personal intersest and is a place to share state across different machines, networks, and devices.

## AI Related

| #             | Topic         | Notes | URL |
| ------------- |-------------| -----|-----|
| 1 | ReCo: Region-Controlled Text-to-Image Generation |  | https://arxiv.org/pdf/2211.15518v1.pdf |
| 2 | VLP: A Survey on Vision-Language Pre-training |    | https://arxiv.org/abs/2202.09061 |
| 3 | Efficient Training of Language Models to Fill in the Middle | FIM    | https://arxiv.org/abs/2207.14255 |
| 4 | Cramming: Training a Language Model on a Single GPU in One Day |     | https://arxiv.org/abs/2212.14034 |
| 5 | DeepSpeed Data Efficiency: A composable library that makes better use of data, increases training efficiency, and improves model quality |     | https://www.deepspeed.ai/2022/12/11/data-efficiency.html |
| 6 | LoRA: Low-Rank Adaptation of Large Language Models |     | https://github.com/microsoft/LoRA |
| 7 | How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection |     | https://arxiv.org/abs/2301.07597 |
| 8 | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | Model parallelism    | https://arxiv.org/abs/1909.08053 |
| 9 | GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism | Model parallelism    | https://arxiv.org/abs/1811.06965 |
| 10 | DeepSpeed: Extreme-scale model training for everyone | Model parallelism    | https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/ |
| 11 | Turing-NLG: A 17-billion-parameter language model by Microsoft | Model parallelism    | https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/ |
| 12 | Talking About Large Language Models | LLMs | https://arxiv.org/abs/2212.03551 |
| 13 | Language Is Not All You Need: Aligning Perception with Language Models | LLMs | https://arxiv.org/abs/2302.14045 |
| 14 | Toolformer: Language Models Can Teach Themselves to Use Tools | LLMs Search | https://arxiv.org/abs/2302.04761 |
| 15 | LoRA: Low-Rank Adaptation of Large Language Models| Foundational models | https://arxiv.org/abs/2302.04761 |
